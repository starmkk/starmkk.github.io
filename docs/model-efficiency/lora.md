---
title: "LoRA: 대규모 언어 모델을 위한 경량 적응 기법"
parent: 모델 경량화
nav_order: 1
layout: default
permalink: /model-efficiency/lora/
mathjax: true
---

# LoRA란 무엇인가

LoRA(Low-Rank Adaptation)는 대규모 언어 모델(LLM)에서 적은 리소스를 활용해 모델을 미세 조정할 수 있도록 고안된 기법이다. 이 기술은 기존 모델의 성능을 유지하며, 학습 가능한 저랭크 행렬만을 조정해 효율성을 극대화한다.

## 연구 배경과 동기

- **대규모 모델의 확산**: GPT-3, PaLM 등 수백억 개 이상의 파라미터를 가진 모델의 등장으로, 특정 도메인/태스크에 맞춘 전통적인 전체 미세 조정은 연산 비용과 저장 공간 측면에서 비현실적으로 커졌다.
- **Multi-Task와 Multi-Domain 요구**: 하나의 기반 모델을 여러 응용 영역에서 재사용해야 하는 상황이 늘어나면서, 서로 다른 설정에 맞춘 파라미터 세트를 효율적으로 관리할 방법이 요구되었다.
- **Adapter 계열 기법의 한계**: 기존 Adapter는 성능은 좋지만 삽입 지점마다 새로운 블록을 추가해야 하므로 추론 지연(latency)과 메모리 사용량이 증가한다. LoRA는 선형 계층(Linear Layer)에 저랭크 행렬을 주입함으로써 더 가벼운 대안이 된다.

## 구조와 동작 원리

### 1. 선형 계층에 대한 저랭크 분해(low rand decomposition)

기존 선형 계층의 가중치 행렬 $W \in \mathbb{R}^{d \times k}$를 유지한 채, LoRA는 학습 가능한 두 행렬 $A \in \mathbb{R}^{r \times k}$와 $B \in \mathbb{R}^{d \times r}$를 추가한다. 여기서 $r$은 랭크(rank)로, 일반적으로 $r \ll \min(d, k)$이다. 순전파 시 입력 $x$에 대해 출력은 다음과 같이 계산된다.

| LoRA 구조 개념도 | 수식 표현 |
| --- | --- |
| ![LoRA 구조 개념도](/assets/images/lora-diagram.png){: width="280" } | $$W x + B A x$$ |

원래 가중치 $W$는 동결되어 있고, 저랭크 행렬 $A$와 $B$만 학습된다. 이 덕분에 업데이트해야 하는 파라미터 수가 $d \times k$에서 $r \times (d + k)$로 줄어든다.

### 2. 스케일링과 초기화

- **스케일링 계수**: 학습 초기에 불안정성을 줄이기 위해 LoRA는 $\frac{\alpha}{r}$ 형태의 스케일 값을 사용한다. 일반적으로 $\alpha$는 16~64 사이로 설정된다.
- **초기화 전략**: \( B \)는 영행렬로 시작하여 초기에 모델의 출력이 기존 모델과 동일하게 유지되도록 한다. \( A \)는 He 또는 Xavier 초기화를 사용하며, 작은 학습률로 안정적으로 훈련한다.

### 3. 적용 위치

- **Self-Attention**: Query, Value 프로젝션 계층에 LoRA를 적용하면, 문맥 이해와 응답 생성이 빠르게 맞춤화된다.
- **Feed-Forward 네트워크**: MLP 계층의 첫 번째 선형 변환에 적용하면 추가적인 표현력 향상을 기대할 수 있다.
- **다중 모달/작업 확장**: 최근에는 이미지-텍스트 멀티모달 모델이나 음성 모델에도 LoRA가 적용되고 있으며, GPU 메모리가 제한된 설정에서도 효과적이다(LoRA 가중치 로딩만으로 새로운 도메인에 빠르게 전환 가능.)

## 적용 효과

- **파라미터 효율성**: 예를 들어 7B 파라미터 모델에서 $r = 8$을 사용하면 추가 학습 파라미터 수가 전체의 0.05% 수준으로 감소한다.
- **메모리 절감**: 파라미터 업데이트를 위한 옵티마이저 상태(예: Adam 모멘트)도 저랭크 행렬에 대해서만 유지하면 되므로 학습 시 메모리 사용량이 크게 줄어든다.
- **빠른 전환(Hot-Swapping)**: 동일한 기반 모델 위에 서로 다른 LoRA 가중치를 로딩하는 것만으로 서비스 도메인을 바꿀 수 있어 운영 측면에서 민첩성이 향상된다.
- **학습 안정성**: 학습률과 스케일링 계수를 적절히 설정하면, 전체 미세 조정 대비 더 적은 에폭으로 수렴하는 경우가 많다.
- **성능 사례**: 원저자는 GPT-3 175B를 대상으로 LoRA를 적용하여 자연어 추론, 질의응답, 요약 등 다양한 태스크에서 전체 미세 조정과 동등하거나 근소하게 높은 성능을 달성했다고 보고했다.

## 실무 적용 체크리스트

1. **랭크와 스케일 선택**: 기본값으로 $r = 8$, $\alpha = 16$ 혹은 32를 사용하고, 데이터셋 크기에 따라 조정한다.
2. **학습률 스케줄**: LoRA 행렬에만 학습률을 적용하므로 1e-4 ~ 5e-4 범위에서 탐색하는 것이 일반적이다.
3. **혼합 정밀도 활용**: FP16/BF16 학습이 LoRA에도 잘 적용되지만, 저랭크 행렬은 FP32로 유지하면 수치 안정성이 높아진다.
4. **추론 배포 전략**: LoRA 가중치를 합친(merged) 형태로 내보내거나, 추론 시 동적으로 주입하는 온더플라이 방식 중 운영 환경에 맞는 방법을 선택한다.
5. **품질 검증**: 전환된 도메인/태스크별로 A/B 테스트를 수행하고, Hallucination, Bias 등 모델 거버넌스 지표를 함께 평가한다.

## 키워드
- **저랭크(Low-Rank) 학습**: $W x + B A x$ 형태로 경량화된 미세 조정 방식.
- **효율성**: 업데이트 파라미터 수 0.05% 수준, 메모리 절감.
- **적용 위치**: Self-Attention, MLP, 다중 도메인 확장.
- **기술적 특징**: $r$, $\alpha$, 스케일링, 초기화 전략.
- **적용 효과**: 빠른 전환(Hot-swapping), 안정적 학습.
- **배경**: 대규모 LLM 확산, Multi-task/Domain 요구 대응, Adapter 한계 극복.



---

### 참고 문헌

- Hu, Edward J., et al. "LoRA: Low-Rank Adaptation of Large Language Models." *arXiv preprint arXiv:2106.09685* (2021).
- Dettmers, Tim, et al. "QLoRA: Efficient Finetuning of Quantized LLMs." *arXiv preprint arXiv:2305.14314* (2023).
- Xu, Canwen, et al. "Parameter-Efficient Fine-Tuning for Speech and Vision Models." *Proceedings of the IEEE/CVF* (2023).